---
title: "Predict_Income_Income_Level"
author: "Steve Dubois"
date: "4/8/2019"
output:
  html_document: default
  pdf_document: default
---
<style type="text/css">

h1.title {
  font-size: 38px;
  color: DarkRed;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}
</style>


##ABSTRACT
The prominent inequality of wealth and income is a huge concern especially in the United States. The likelihood of diminishing poverty is one valid reason to reduce the world's surging level of economic inequality. The principle of universal moral equality ensures sustainable development and improve the economic stability of a nation. Governments in different countries have been trying their best to address this problem and provide an optimal solution. This study aims to show the usage of machine learning and data mining techniques in providing a solution to the income equality problem. The UCI Adult Dataset has been used for the purpose.  Machine learning classification has been done to predict whether a person's yearly income in US falls in the  income category of either greater than 50K Dollars or less equal to 50K Dollars category based on a certain set of attributes. 

<style type="text/css">

h1.title {
  font-size: 38px;
  color: DarkRed;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}
</style>

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r, echo = TRUE, message=FALSE, warning=FALSE}
  
  library(ISLR)
  library(gmodels)  
  library(broom)
  library(tidyverse)
  library(tidyr)
  library(dplyr)
  library(readr)
  library(ggplot2)
  library(randomForest)
  library(caret)
  library(xgboost)
  library(mlr)
  library(optparse)
  library(data.table)
  library(ggplot2)
  library(gbm)
  library(ggpubr)
  library(rpart)
  library(e1071)
  library(rpart.plot)
  library(plotly)
  library(rattle)
  library(GGally)
  library(neuralnet)
  library(nnet)
  library(network)
  library(networkD3)
```



#Loading the Census Data
```{r, echo = TRUE}
option_list <- list(
  make_option("--train", type = "character",
              default = "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"),
  make_option("--test", type = "character",
              default = "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"))

opt <- parse_args(OptionParser(option_list = option_list))
link_train <- opt$train
link_test <- opt$test



#Read the files using the links from args
train <-  read_csv(file = link_train, skip = 1, col_names = TRUE)
test <-  read_csv(file = link_test, skip = 1, col_names = TRUE)
```




```{r, echo = TRUE, message=FALSE, warning=FALSE}
#read the files using the links from args
train <-  read_csv(file = link_train, skip = 1)
test <-  read_csv(file = link_test, skip = 1)
```



```{r, echo = TRUE, message=FALSE, warning=FALSE}
#Initializing headers
headings <- c("Age","Work_Class", "Final_Weight", "Education", "Education_Num", "Marital_Status", "Occupation",   "Relationship", "Race", "Sex", "Capital_Gain", "Capital_Loss", "Hours_Per_Week", "Native_Country", "IncomeCLASS")

#Adding headers
names(train) <- headings
names(test) <- headings
train <- as.data.frame(train)
test <- as.data.frame(test)
train$IncomeCLASS <- as.factor(train$IncomeCLASS)
test$IncomeCLASS <- as.factor(test$IncomeCLASS)
levels(train$IncomeCLASS) <- c("Less_50K", "More_50K")
levels(test$IncomeCLASS) <- c("Less_50K", "More_50K")


```



```{r, echo=TRUE, message=FALSE, warning=FALSE}
impute.mean <- function(x) replace(x, is.na(x) | is.nan(x) | is.infinite(x), mean(x[!is.na(x) & !is.nan(x) & !is.infinite(x)]))
losses <- apply(train, 2, impute.mean)
sum(apply( losses, 2, function(.) sum(is.infinite(.)) ))

impute.mean <- function(x) replace(x, is.na(x) | is.nan(x) | is.infinite(x), mean(x[!is.na(x) & !is.nan(x) & !is.infinite(x)]))
losses <- apply(test, 2, impute.mean)
sum(apply( losses, 2, function(.) sum(is.infinite(.)) ))
```



###Setting Categorical Features/Variabls 
```{r, echo = FALSE, message=FALSE, warning=FALSE}
train$Native_Country <- as.factor(train$Native_Country)
train$Work_Class <- as.factor(train$Work_Class)
train$Education <- as.factor(train$Education)
train$Marital_Status <- as.factor(train$Marital_Status)
train$Occupation <- as.factor(train$Occupation)
train$Relationship <- as.factor(train$Relationship)
train$Race <- as.factor(train$Race)
train$Sex <- as.factor(train$Sex)


test$Native_Country <- as.factor(test$Native_Country)
train$Work_Class <- as.factor(train$Work_Class)
test$Work_Class <- as.factor(test$Work_Class)
test$Education <- as.factor(test$Education)
test$Marital_Status <- as.factor(test$Marital_Status)
test$Occupation <- as.factor(test$Occupation)
test$Relationship <- as.factor(test$Relationship)
test$Race <- as.factor(test$Race)
test$Sex <- as.factor(test$Sex)
```



#Exploratory Data Analysis Using GGplot
```{r, echo = TRUE, message=FALSE, warning=FALSE}
P <- ggplot(train,aes(x = Age, fill = IncomeCLASS)) + geom_bar(position = "fill", color = "black") + coord_flip()

P + labs(title = "Age vs Income Class Proportion")
```



```{r, echo = TRUE, message=FALSE, warning=FALSE}
  

Q <- ggplot(train,aes(x = Education, fill = IncomeCLASS)) + geom_bar() + coord_flip()
Q + labs(title = "Education vs Income Class Proportion")

```



```{r, echo = TRUE, message=FALSE, warning=FALSE}
 ggplot(train,aes(x = Work_Class, fill = IncomeCLASS)) + geom_bar(position = "fill", color = "black") + coord_flip()
```



```{r, echo = TRUE, message=FALSE, warning=FALSE}
 ggplot(train,aes(x = Education_Num, fill = IncomeCLASS)) + ggtitle("Length of Eduction VS Income Class Proportion") + xlab("Years of Education") + ylab("proportion within category") + geom_bar(position = "fill", color = "black") + coord_flip()

```



```{r, echo = TRUE, message=FALSE, warning=FALSE}
 qplot (IncomeCLASS, data = train, fill = Relationship) + facet_grid (. ~ Relationship)


 ggplot(train,aes(x = IncomeCLASS, fill = Relationship)) + ggtitle("Relationship VS Income Class Proportion") + xlab("Income Class") + ylab("proportion within category") + geom_bar(position = "fill", color = "black") + coord_flip()

```



```{r, echo = TRUE, message=FALSE, warning=FALSE}
 qplot (Occupation, data = train, fill = IncomeCLASS) 

ggplot(train,aes(x = Occupation, fill = IncomeCLASS)) + ggtitle("Length of Eduction VS Income Class Proportion") + xlab("Years of Education") + ylab("proportion within category") + geom_bar(position = "fill", color = "black") + coord_flip()
```



##LOGISTIC REGRESSION MODEL
```{r, echo = TRUE, message=FALSE, warning=FALSE}
# Logistic Regression Model
set.seed (32323)
trCtrl = trainControl (method = "cv", number = 10)
model_logit <- caret::train(IncomeCLASS ~., data = train, method = "glm", tuneGrid=expand.grid(parameter=c(0.001, 0.01, 0.1, 1,10,100, 1000)))
pred_logit <- predict(model_logit, test)
confusionMatrix(pred_logit, test$IncomeCLASS)
CrossTable(pred_logit, test$IncomeCLASS)
logit_VarImp <- varImp(model_logit, scale = FALSE)

```



##NAIVE BAYES MODEL
```{r, echo = TRUE, message=FALSE, warning=FALSE}
#train Naive Bayes
model_Naive <- naiveBayes(IncomeCLASS ~ ., data = train)
pred_Nb <- predict(model_Naive, test)
confusionMatrix(pred_Nb, test$IncomeCLASS)
CrossTable(pred_Nb, test$IncomeCLASS)
summary(pred_Nb)
```



##TRAIN THE RPART DECISION TREE MODEL
```{r, echo = TRUE, message=FALSE, warning=FALSE}
# rpart decision tree 
set.seed(32323)
V <- 10
T <- 4
TrControl <- trainControl(method = "repeatedcv",
                          number = V,
                          repeats = T)


model_part <- caret::train(IncomeCLASS ~., data = train, method = "rpart",  control = rpart::rpart.control(minsplit = 5, cp = 0), tuneGrid = data.frame(cp = .02), trControl = TrControl)
pred_rpart <- predict(model_part, test, type = "raw")

confusionMatrix(pred_rpart, test$IncomeCLASS)
CrossTable(pred_rpart, test$IncomeCLASS)
summary(pred_rpart)

model_part$finalModel

rpart.plot(model_part$finalModel)

```



##TRAIN THE RANDOM FOREST MODEL
```{r, echo = TRUE, message=FALSE, warning=FALSE}
levels(test$Native_Country) <- levels(train$Native_Country)
model.rf <- randomForest(IncomeCLASS~., data = train, ntree=500, importance=TRUE)
pred.rf <- predict(model.rf, test)
summary(pred.rf)
confusionMatrix(test$IncomeCLASS, pred.rf)
plot(model.rf)
```



##TRAIN THE SVM MODEL
```{r, echo = TRUE, message=FALSE, warning=FALSE}
# train the SVM model
set.seed(32323)
library(e1071)
library(rpart)
svm.model <- caret::train(IncomeCLASS~., data = train, method="svmRadial", trControl = trCtrl , verbose = TRUE)
pred.svm <- predict(svm.model, test)
confusionMatrix(pred.svm, test$IncomeCLASS)
CrossTable(pred.svm, test$IncomeCLASS)
```



## NEURAL NETWORKS
```{r, echo = TRUE, message=FALSE, warning=FALSE}
## NEURAL NETWORKS

# define training control

train_control <- trainControl(method = 'repeatedcv', number = 10, classProbs = TRUE, verboseIter = TRUE, preProcOptions = list(thresh = 0.75, ICAcomp = 3, k = 5))

# train the model

metric <- "Accuracy"

model_neural <- caret::train(IncomeCLASS ~ ., data = train, method = "nnet", MaxNWts = 100000,  metric=metric, preProcess = c('center', 'scale'), trControl = train_control, linout=FALSE)


# summarize results
print(model_neural)
```



```{r, echo=TRUE}
# Confusion matrix
pred_neural <- predict(model_neural, test, type = "raw")
confusionMatrix(pred_neural, test$IncomeCLASS)
CrossTable(pred_neural, test$IncomeCLASS)
```


## CONCLUSIONS:
The analysis confirmed (and quantified) what is considered common sense:

Age, education, occupation, and marital status (or relationship kind) are good for predicting income (above a certain threshold).

(1) if a person earns more than $50000 he is very likely to be a married man with large number of years of education;
(2) single parents, younger than 25 years, who studied less than 10 years, and were never-married make less than $50000.




